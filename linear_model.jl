using Random
using LinearAlgebra
"""
Initilize, update and get action from linear model
"""

scale_gradient(âˆ‡, L2_max) = min(L2_max/norm(âˆ‡), 1)*âˆ‡

mutable struct EpsilonGreedyExploration
    Ïµ # probability of random arm
end

function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ = model.ğ’œ, Ï€.Ïµ
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), ğ’œ)
end

mutable struct SoftmaxExploration
    Î» # precision parameter
    Î± # precision factor
end

function (Ï€_sm::SoftmaxExploration)(model, s)
    Î», Î± = Ï€_sm.Î», Ï€_sm.Î±
    Q(s,a) = lookahead(model, s, a)
    weights = exp.(Î» * mean.([Q(s,a) for a in ğ’œ]))
    Î» *= Î±
    return rand(Categorical(normalize(weights, 1)))
end

struct GradientQLearning
    ğ’œ  # action space (assumes 1:nactions)
    Î³  # discount
    Q  # parameterized action value function Q(Î¸,s,a)
    âˆ‡Q # gradient of action value function
    Î¸  # action value function parameter
    Î±  # learning rate
end

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    u = maximum(Q(Î¸,sâ€²,aâ€²) for aâ€² in ğ’œ)
    println("Q($s, $a) before = ", Q(Î¸,s,a))
    Î” = (r + Î³*u - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
    println("Î” = $Î”")
    Î¸[:] += Î±*scale_gradient(Î”, 1)    
    for a in model.ğ’œ
        println("Q($s, $a) = after", Q(Î¸,s,a))
    end
    return model
end

function create_model(dim_ğ’®, num_ğ’œ)
    """
    Initilizes linear model with zero weights
        @param dim_ğ’®: dimension of a singular state
        @param num_ğ’œ: size of action space
        @return model
    """
    Î²(s,a) = [a,a^2,1] #[s; s.^2]#; [a,a^2,1] ]
    #Î²(s,a) = [s;sin.(s*a);[a, 1/(a^2), 1]]
    Q(Î¸,s,a) = dot(Î¸,Î²(s,a))
    âˆ‡Q(Î¸,s,a) = Î²(s,a)

    # edit size when changing Î²
    Î¸ = zeros(3)#2*dim_ğ’®)#+3) # initial parameter vector 
    Î± = 0.1 # learning rate
    ğ’œ = collect(1:num_ğ’œ) # number of states = 12
    Î³ = 0.95 # discount
    model = GradientQLearning(ğ’œ, Î³, Q, âˆ‡Q, Î¸, Î±) 
    return model
end

function get_action(model::GradientQLearning, s, rand, test)
    """
    Returns action for current state s
        @param model: initilized linear model 
        @param s: current state
        @param rand: explore only << initializing phase >> 
        @param test: exploit only << testing phase >> 
        @return action
    """
    Îµ = rand ? 1 : (test ? 0 : 0.1) # probability of random action
    Ï€ = EpsilonGreedyExploration(Îµ)
    return Ï€(model, s)
end

function get_action_sm(model::GradientQLearning, s, learn, test)
    """
    Returns action for current state s
        @param model: initilized linear model 
        @param s: current state
        @param learn: explore only << initializing phase >> 
        if learn is true, Î» is 1
        @param test: exploit only << testing phase >> 
        if learn is false
            if test is true , Î» is 50 (0 is uniform)
            else if test is false, print error
        @return action
    """
    if learn
        Î» = 50 # probability of random action
        Î± = 1 #1 is no decay to start
        Ï€_sm = SoftmaxExploration(Î», Î±)
        return Ï€_sm(model, s)
    elseif test
        Q(s,a) = lookahead(model, s, a)
        # for a in model.ğ’œ
        #     println("Q($s, $a) = ", Q(s, a))
        # end
        return argmax(a->Q(s,a), model.ğ’œ)
    else
        println("did not specify learn or test, one must be set to True")
    end
end

function example_run()
    """
    Demos how to use
    """
    model = create_model(6, 3)
    for i in 1:1000
        s = rand(6,1) * 100
        a = rand(1:3)
        r = rand() * 10
        sâ€² = rand(6,1) * 100
        update!(model, s, a, r, sâ€²)
    end
    print(model.Î¸)
    for i in 1:10
        println(get_action_sm(model, rand(6,1) * 100, false, true))
    end
end