using Random
using LinearAlgebra
using Statistics
using Distributions
"""
Initilize, update and get action from linear model
"""

scale_gradient(âˆ‡, L2_max) = min(L2_max/norm(âˆ‡), 1)*âˆ‡

mutable struct EpsilonGreedyExploration
    Ïµ # probability of random arm
    Î± # decay constant between 0-1
end

function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ, Î± = model.ğ’œ, Ï€.Ïµ, model.Î±
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Ïµ *= Î±
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), ğ’œ)
end

mutable struct SoftmaxExploration
    Î» # precision parameter, choose small value so it doesn't go to infinity
    Î± # precision factor
end

function (Ï€::SoftmaxExploration)(model, s)
    ğ’œ, Î», Î± = model.ğ’œ, Ï€.Î», Ï€.Î±
    Q(s,a) = lookahead(model, s, a)
    weights = exp.(Î» * ([Q(s,a) for a in ğ’œ]))
    Î» *= Î±
    weights = normalize(weights, 1)
    p = (1-sum(A -> map(x -> isnan(x) ? zero(x) : x, A), weights)) / count(i->(isnan(i)), weights)
    replace!(weights, NaN=>p)
    return rand(Categorical(weights))
end

mutable struct UCB1Exploration
    c # exploration constant
end

function bonus(counts, a)
	N = sum(counts)
	Na = counts[a]
    return sqrt(log(N)/Na)
end

function (Ï€::UCB1Exploration)(model, s)
    Q(s,a) = lookahead(model, s, a)
    Ï = [Q(s,a) for a in model.ğ’œ]
    u = Ï .+ Ï€.c*[bonus(model.N, a) for a in model.ğ’œ]
    return argmax(u)
end

"""
DOES NOTE WORK AS IT IS RIGHT NOW
since Q(s,a) is a scalar, not a vector or distribution 

mutable struct QuantileExploration
    Î± # quantile (e.g., 0.95)
end

function (Ï€::QuantileExploration)(model, s)
    Q(s,a) = lookahead(model, s, a)
    return argmax(quantile(a->Q(s,a), Ï€.Î±), ğ’œ)
end
"""

struct GradientQLearning
    ğ’œ  # action space 
    Î³  # discount
    Q  # parameterized action value function Q(Î¸,s,a)
    âˆ‡Q # gradient of action value function
    Î¸  # action value function parameter
    Î±  # learning rate
    N  # number of times action was taken + pseudocounts
end

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î±, N = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±, model.N   
    u = maximum(Q(Î¸,sâ€²,aâ€²) for aâ€² in ğ’œ) #picks the right Î¸ from create model
    Î” = (r + Î³*u - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
    Î¸[a,:] += Î±*scale_gradient(Î”, 1)
    N[a] += 1
    return model
end

function Î²_helper(s,a,ğ’œ)
    a = Dict(collect(1:length(ğ’œ)).=> ğ’œ)[a]
    return [s;s.^3;[1/a]]
end

function create_model(dim_ğ’®,ğ’œ)
    """
    Initilizes linear model with zero weights
        @param dim_ğ’®: dimension of a singular state
        @param ğ’œ: action space
        @return model
    """
    # refer to line 115 in framework.py
    Î²(s,a) = Î²_helper(s,a,ğ’œ)

    Q(Î¸,s,a) = dot(Î¸[a,:],Î²(s,a))
    âˆ‡Q(Î¸,s,a) = Î²(s,a)

    # edit size when changing Î²
    Î¸ = zeros(length(ğ’œ), 2*dim_ğ’®+1) #initial parameter vector
    Î± = 0.5 # learning rate
    ğ’œâ€² = collect(1:length(ğ’œ)) # number of states
    Î³ = 0.95 # discount
    N = ones(length(ğ’œ)) # pseudocounts for UCB1
    model = GradientQLearning(ğ’œâ€², Î³, Q, âˆ‡Q, Î¸, Î±, N)
    return model
end

function get_action(model::GradientQLearning, Ï€, s, rand, test)
    """
    Returns action for current state s
        @param model: initilized linear model
        @param Ï€: exploration policy
        @param s: current state
        @param rand: explore only (FULLY random) << initializing phase >>
        @param test: exploit only << testing phase >>
        @return action
    """
    if rand
        return rand(model.ğ’œ)
    elseif test
        Q(s,a) = lookahead(model, s, a)
        return argmax(a->Q(s,a), model.ğ’œ)
    end
    return Ï€(model, s)
end

function example_run()
    """
    Demos how to use
    """
    model = create_model(18,[60,30,45])
    exploration_policy = UCB1Exploration(1)
    for i in 1:1000
        s = rand(18,1) * 100
        a = rand(1:3)
        r = rand() * 10
        sâ€² = rand(18,1) * 100
        update!(model, s, a, r, sâ€²)
    end
    # print(model.Î¸)
    for i in 1:10
        println(get_action(model, exploration_policy, rand(18,1) * 100, false, false))
    end
end